model_list:
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: azure/GPT-35-TURBO
      api_key: os.environ/AZURE_API_KEY
      api_base: https://gpt4-preview.openai.azure.com/
  - model_name: gpt-4
    litellm_params:
      model: azure/GPT4-32K
      api_key: os.environ/AZURE_API_KEY
      api_base: https://gpt4-preview.openai.azure.com/
  - model_name: Llama-2-13b-chat-hf # the 1st model is the default on the proxy
    litellm_params: # params for litellm.completion() - https://docs.litellm.ai/docs/completion/input#input---request-body
      model: huggingface/meta-llama/Llama-2-13b-chat-hf
      # api_base: http://host.docker.internal:8080
      api_base: http://10.189.3.205:8080

litellm_settings:
  drop_params: True
  set_verbose: True
  add_function_to_prompt: True
  success_callback: ["langfuse"]



general_settings: 
  # infer_model_from_keys: False

# Start up proxy: 
# poetry add langfuse
# poetry run litellm --config proxy_hf_config.yaml